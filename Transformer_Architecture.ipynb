{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e069a012-edb2-4f99-a1be-c1d1ee01b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "random.seed(220)\n",
    "torch.manual_seed(220);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752e9da5-0be6-4f89-9b95-d2d81ddeeb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971edaef-72a3-4d39-9c58-80cc66666f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words_250000_train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf7aa05-7256-4be6-bce7-a360a5e81edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brace',\n",
       " 'wimblelike',\n",
       " 'postcecal',\n",
       " 'deferable',\n",
       " 'egesting',\n",
       " 'gnashed',\n",
       " 'camister',\n",
       " 'busti',\n",
       " 'establishments',\n",
       " 'respectlessness']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(text)\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e63ff9a-8fc8-4a92-9458-ef4567aab4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max(len(words) for words in text)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5be3ef-35ec-4c72-96cf-a1ca69c89923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 28\n"
     ]
    }
   ],
   "source": [
    "charset = sorted(list(set(''.join(text))))\n",
    "stoi = {c:idx+2 for idx,c in enumerate(charset)}\n",
    "itos = {idx+2:c for idx,c in enumerate(charset)}\n",
    "stoi['.'] = 1 \n",
    "itos[1] = '.'\n",
    "stoi[''] = 0\n",
    "itos[0] = ''\n",
    "encode = lambda x: [stoi[i] for i in x] \n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "vocab_size = len(stoi)\n",
    "print(f'{vocab_size = }') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd3e413d-b68f-4094-9d48-8c9521d95a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word = 'fichte'\n"
     ]
    }
   ],
   "source": [
    "word = random.sample(text, 1)[0]\n",
    "print(f'{word = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df89cd68-d651-4470-8813-a4f7154d0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded = [7, 10, 4, 9, 21, 6]\n"
     ]
    }
   ],
   "source": [
    "encoded = encode(word)\n",
    "print(f'{encoded = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "340bf1e5-64d5-4b4f-8b36-c455ffe773ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded = 'fichte'\n"
     ]
    }
   ],
   "source": [
    "decoded = decode(encoded)\n",
    "print(f'{decoded = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5024da87-e228-44c1-8386-111e5e5ed338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    x_temp = []\n",
    "    y = []\n",
    "    for word in words:\n",
    "        encoded = np.array(encode(word))\n",
    "        input = np.ones_like(encoded)\n",
    "        x_temp.append(input) \n",
    "        sorted_letter_count = Counter(word).most_common()\n",
    "        g = [0]*26\n",
    "        prev = x_temp[-1].copy()\n",
    "        while not np.all(prev != 1):\n",
    "            prev = x_temp[-1].copy()\n",
    "            for letter, _ in sorted_letter_count:\n",
    "                idx = ord(letter) - ord('a')\n",
    "                if g[idx] != 1:\n",
    "                    g[idx] = 1\n",
    "                    g_in = stoi[letter]\n",
    "                    break\n",
    "            y.append(g_in)\n",
    "            idxs = np.where(encoded == g_in)[0]\n",
    "            prev[idxs] = g_in \n",
    "            if np.any(prev == 1):\n",
    "                x_temp.append(prev)\n",
    "            \n",
    "    # adding paddding to every input \n",
    "    x = []\n",
    "    for ix in x_temp:\n",
    "        extra = maxlen - len(ix)\n",
    "        pad = np.array([0]*extra)\n",
    "        x_in = np.concatenate((ix, pad))\n",
    "        x.append(x_in)\n",
    "    \n",
    "    x = torch.tensor(np.array(x), dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    return x,y \n",
    "\n",
    "random.shuffle(text)\n",
    "n1 = int(0.9*len(text))\n",
    "n2 = int(0.95*len(text))\n",
    "\n",
    "# Xtr, Ytr = build_dataset(text[:n1])\n",
    "# Xval, Yval = build_dataset(text[n1:n2])\n",
    "# Xts, Yts  = build_dataset(text[n2:])\n",
    "\n",
    "# load dataset\n",
    "def load_split(split):\n",
    "    try:\n",
    "        with open(f'data\\X_{split}.pkl', 'rb') as f:\n",
    "            X = pickle.load(f)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    try:\n",
    "        with open(f'data\\Y_{split}.pkl', 'rb') as f:\n",
    "            Y = pickle.load(f)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    return X, Y \n",
    "\n",
    "Xtr, Ytr = load_split('train')\n",
    "Xval, Yval = load_split('val')\n",
    "Xts, Yts = load_split('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8bef7b-ab1c-4a88-8ff0-8dfbc43953c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : torch.Size([1513485, 29]) torch.Size([1513485])\n",
      "Val Shape : torch.Size([84078, 29]) torch.Size([84078])\n",
      "Test Shape : torch.Size([83646, 29]) torch.Size([83646])\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Shape : {Xtr.shape} {Ytr.shape}')\n",
    "print(f'Val Shape : {Xval.shape} {Yval.shape}')\n",
    "print(f'Test Shape : {Xts.shape} {Yts.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbde4ea-1ca9-43ff-b695-d92192b4efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data  = {'train': (Xtr, Ytr),\n",
    "            'val' : (Xval, Yval)}[split]\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "    xb, yb = Xtr[ix].to(device), Ytr[ix].to(device)\n",
    "    return xb, yb\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(200)\n",
    "        for k in range(200):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb,yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a7c426-d45c-4eaf-856a-3e1ec09d4930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparemters \n",
    "learning_rate = 1e-3\n",
    "n_embd = 32\n",
    "n_hidden = 100\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79c8585-4031-4bda-b51c-768c6e29d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__() \n",
    "        self.key = nn.Linear(n_embd, head_size)\n",
    "        self.query = nn.Linear(n_embd, head_size)\n",
    "        self.value = nn.Linear(n_embd, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C **-0.5\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        v = self.value(x)\n",
    "\n",
    "        out = wei @ v  # 32, 29, 32 (batch, seq_len, n_embd)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e03f96-a2b4-427e-913e-eba5ddcfe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21237dfc-461b-4907-b490-cd86b8c78127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x) # B, T, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b16d09db-bc4c-43b7-8a2d-eb736f2df01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.ma = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ma(self.ln1(x))\n",
    "        x = self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e80687-33e7-405a-8af0-f5102ef9e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_table = nn.Embedding(maxlen, n_embd)\n",
    "        self.block = nn.Sequential(\n",
    "            Block(n_embd, n_heads=4),\n",
    "            Block(n_embd, n_heads=4),\n",
    "            Block(n_embd, n_heads=4),\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding_table(x)\n",
    "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.block(x)\n",
    "        x = x.view(B, -1)\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        return logits, loss \n",
    "\n",
    "    def guessletter(self, context, guessed_letters):\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self(context)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            while guessed_letters[ix-2] == 1:\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2638b1f1-51cb-45be-8c7d-7262b7613b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef09acb4-ea2e-4228-a81f-205eade96965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40988"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters(recurse=True))\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b97a50d-20c2-4301-974c-18b87fd8ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.8488, val loss 2.8814\n",
      "step 1000: train loss 2.8405, val loss 2.8322\n",
      "step 2000: train loss 2.8064, val loss 2.8036\n",
      "step 3000: train loss 2.7943, val loss 2.7708\n",
      "step 4000: train loss 2.7787, val loss 2.7744\n",
      "step 5000: train loss 2.7590, val loss 2.7583\n",
      "step 6000: train loss 2.7413, val loss 2.7416\n",
      "step 7000: train loss 2.7222, val loss 2.7430\n",
      "step 8000: train loss 2.7309, val loss 2.7426\n",
      "step 9000: train loss 2.7504, val loss 2.7312\n",
      "step 10000: train loss 2.7167, val loss 2.7275\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = split_loss()\n",
    "        print(f\"step {epoch}: train loss {loss['train']:.4f}, val loss {loss['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "loss = split_loss()\n",
    "print(f\"step {epochs}: train loss {loss['train']:.4f}, val loss {loss['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681cb44-a049-4e2c-99c0-08ca2159040b",
   "metadata": {},
   "source": [
    "First Run (scratch implementation) -- 8784\n",
    "- 'train' : 2.9786,  'val' : 2.9790\n",
    "\n",
    "Pytorch Implementation (better optimizer, better weight initializations) -- 8784\n",
    "- train loss 2.6329, val loss 2.63234\n",
    "\n",
    "Deeper NN (n_embd = 32, n_hidden = 200) -- 193K\n",
    "- train loss 2.2853, val loss 2.2794\n",
    "  \n",
    "Added positional encoding -- 193K\n",
    "- train loss 2.3121, val loss 2.2749\n",
    "\n",
    "added self attention -- 196k\n",
    "- step 100000: train loss 2.2082, val loss 2.2056\n",
    "\n",
    "Transformer architecture (1 block)(multiheadattention) -- 40K\n",
    "- train loss 2.1944, val loss 2.1841\n",
    "\n",
    "Transformer (n_embd = 64) -- 100K\n",
    "- train loss 2.1346, val loss 2.1468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6353cf5d-3e95-42bc-b047-d32ede5f9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(word):\n",
    "    input = [1] * len(word)\n",
    "    encoded = encode(word)\n",
    "    pad = [0] * (maxlen - len(word))\n",
    "    chances = 6\n",
    "    guessed_letters = [0]*26 \n",
    "    context = input + pad\n",
    "    context = torch.tensor([context]).to(device)\n",
    "    while chances > 0:\n",
    "        ix = m.guessletter(context, guessed_letters)\n",
    "        guessed_letters[ix-2] = 1 \n",
    "        idx = np.where(np.array(encoded) == ix)[0]\n",
    "        if idx.size != 0:\n",
    "            context[:, idx] = ix\n",
    "        else:\n",
    "            chances -= 1\n",
    "        w = np.array(context.view(-1).cpu())\n",
    "        print(f'Guessed --- {itos[ix]} for Context {decode(w)}') \n",
    "        if decode(w) == word:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b2c1f72-11eb-4445-b586-6f84a005bcba",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "prob_dist must be 1 or 2 dim",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhangman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m, in \u001b[0;36mplay\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      8\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([context])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m chances \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     ix \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguessletter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguessed_letters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     guessed_letters[ix\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m     12\u001b[0m     idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39marray(encoded) \u001b[38;5;241m==\u001b[39m ix)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[18], line 34\u001b[0m, in \u001b[0;36mModel.guessletter\u001b[1;34m(self, context, guessed_letters)\u001b[0m\n\u001b[0;32m     32\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(context)\n\u001b[0;32m     33\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m guessed_letters[ix\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     36\u001b[0m     ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: prob_dist must be 1 or 2 dim"
     ]
    }
   ],
   "source": [
    "play('hangman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d794331-4b62-47f6-ae32-3768a90712d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
